{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the essential libraries\n",
    "import collections\n",
    "from decimal import Decimal\n",
    "import dill as pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL_VARIABLES\n",
    "engFileName='final_eng.txt'\n",
    "dutchFileName='final_dutch.txt'\n",
    "inputEnglishDataset = \"./English_Updated.txt\" #src\n",
    "inputdutchDataset = \"./Dutch_Updated.txt\"\n",
    "size=100000\n",
    "iterations = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Tokenizes the sentence into list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Converts sentence to list of words:\n",
    "    Args: sentence: str\n",
    "    Returns list of words\n",
    "    \"\"\"\n",
    "    consonants = \"bcdfghjklmnpqrstvwxyz\"\n",
    "    # remove the punctuaations, just take the [a-zA-Z0-9]+ type of regex\n",
    "    words=re.split(r'[` \\t\\-=~!@#$%^&*()_+\\[\\]{};\\\\\\:\"|<,./<>?,\\n\\']', sentence)\n",
    "    output = list()\n",
    "    for w in words:\n",
    "        # remove the consonetnts if any\n",
    "        if len(w) == 1 and w.lower() in list(consonants):\n",
    "            continue\n",
    "        if w in [''] and w.isdigit():\n",
    "            continue\n",
    "        w = w.strip()\n",
    "        output.append(w.lower())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning the input files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningInputFiles():\n",
    "    \"\"\"\"\n",
    "    Takes only [a-zA-Z]+ regex words from the sentences\n",
    "    Opens the file and creates new file as \n",
    "    \"\"\"\n",
    "    def readFile(filename):\n",
    "        sentences = list()\n",
    "        try:\n",
    "            with open(filename, \"r\") as fo:\n",
    "                sentences = fo.readlines()\n",
    "            return sentences\n",
    "        except FileNotFoundError:\n",
    "            print(\"Please mention the file correctly!! Current filename: {}\".format(filename))\n",
    "            raise FileNotFoundError\n",
    "        \n",
    "    def clean(outfileName, sen_list):\n",
    "        with open(outfileName, \"w\") as outf:\n",
    "            for line in sen_list:\n",
    "                line = re.sub(r'[^a-zA-Z]', \" \", line)\n",
    "                words = [w for w in tokenize(line) if len(w) > 0]\n",
    "                sentence= \" \".join(words)\n",
    "                outf.write(sentence + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        english = readFile(inputEnglishDataset)\n",
    "        dutch = readFile(inputdutchDataset)\n",
    "    except:\n",
    "        print(\"Something went wrong\")\n",
    "        return None\n",
    "        \n",
    "    clean(engFileName, english)\n",
    "    clean(dutchFileName, dutch)\n",
    "cleaningInputFiles()\n",
    "# 5:15-> 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifiying the collections.defaultdict\n",
    "class customDict(collections.defaultdict):\n",
    "    def __missing__(self, key):\n",
    "        if self.default_factory is None:\n",
    "            raise KeyError(key)\n",
    "        else:\n",
    "            ret = self[key] = self.default_factory(key)\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux(key):\n",
    "    eng_ptr, du_ptr, eng_len, du_len = key\n",
    "    return (1.0/du_len)\n",
    "\n",
    "def _constant_factory(value):\n",
    "    return lambda: value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibmModel1Train(english, dutch, transition_prob, iterations):\n",
    "    print(\"In Model1....\")\n",
    "    for i in range(iterations):\n",
    "        print(\"Running Iteration {}.......: \".format(i+1))\n",
    "        count=collections.defaultdict(float)\n",
    "        total=collections.defaultdict(float)\n",
    "        count=collections.defaultdict(float)\n",
    "\n",
    "        sum_total={}\n",
    "\n",
    "        for(english,dutch) in zip(english, dutch):\n",
    "            english = tokenize(english)\n",
    "            dutch = tokenize(dutch)\n",
    "            for e in english:\n",
    "                sum_total[e]=0.0\n",
    "                for f in dutch:\n",
    "                    sum_total[e]+=trans_prob[(e,f)]\n",
    "\n",
    "            for e in english:\n",
    "                for f in dutch:\n",
    "                    count[(e, f)] += trans_prob[(e, f)] / sum_total[e]\n",
    "                    total[f]+=trans_prob[(e,f)]/sum_total[e]\n",
    "\n",
    "        for (e,f) in count.keys():\n",
    "            transition_prob[(e,f)]=count[(e,f)]/total[f]\n",
    "        \n",
    "        outfile = open('output/new_map1_{}_{}.pickle'.format(str(size, i+1)), 'wb')\n",
    "        pickle.dump(trans_prob, outfile)\n",
    "\n",
    "    return transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibmModel2Train(english, dutch, mapper, iters):\n",
    "    align = customDict(aux)\n",
    "    print(\"In Model1....\")\n",
    "    for x in range(iters):\n",
    "        print(\"Running Iteration: {}.....\".format(x+1))\n",
    "        count_map = collections.defaultdict(float)\n",
    "        count_align = collections.defaultdict(float)\n",
    "        total_map = collections.defaultdict(float)\n",
    "        total_align = collections.defaultdict(float)\n",
    "        total_map_s = collections.defaultdict(float)\n",
    "\n",
    "        for (eng, du) in zip(english, dutch):\n",
    "            eng = tokenize(eng)\n",
    "            du = tokenize(du)\n",
    "            eng_len = len(english)\n",
    "            du_len = len(dutch)\n",
    "            for eng_ptr, eng_word in enumerate(eng, 1):\n",
    "                total_map_s[eng_word] = 0\n",
    "                for ptr, word in enumerate(du, 1):\n",
    "                    total_map_s[eng_word] += mapper[(eng_word, word)] * align[(eng_ptr, ptr, eng_len, du_len)]\n",
    "\n",
    "            for eng_ptr, eng_word in enumerate(eng, 1):\n",
    "                for ptr, word in enumerate(du, 1):\n",
    "                    temp = mapper[(eng_word, word)] * align[(eng_ptr, ptr, eng_len, du_len)] / total_map_s[eng_word]\n",
    "                    count_map[(eng_word, word)] += temp\n",
    "                    total_map[word] += temp\n",
    "                    count_align[(eng_ptr, ptr, eng_len, du_len)] += temp\n",
    "                    total_align[(eng_ptr, eng_len, du_len)] += temp\n",
    "\n",
    "        # update mapper\n",
    "        for key in count_map.keys():\n",
    "            try:\n",
    "                mapper[key] = count_map[key] / total_map[key[1]]\n",
    "            except decimal.DivisionByZero:\n",
    "                print('Error at', key)\n",
    "                continue\n",
    "\n",
    "        #update aligment\n",
    "        for key in count_align.keys():\n",
    "            align[key] = count_align[key] / total_align[(key[0], key[2], key[3])]\n",
    "\n",
    "        pickle.dump(mapper, open('output/new_map2_{}_{}.pickle'.format(size, x+1),'wb'))\n",
    "        pickle.dump(align, open('output/new_align_{}_{}.pickle'.format(size, x+1),'wb'))\n",
    "\n",
    "\n",
    "    return (mapper, align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIBM1_2Model():\n",
    "    english = list()\n",
    "    dutch = list()\n",
    "    with open(engFileName,'r') as inp:\n",
    "        english=inp.readlines()[:size]\n",
    "\n",
    "    with open(dutchFileName,'r') as inp:\n",
    "        dutch=inp.readlines()[:size]\n",
    "\n",
    "    print(\"Training starts for total {} sentences\".format(len(english)))\n",
    "    print(\"Pre-trained IBMModel1 not found..start training\")\n",
    "    mapper = collections.defaultdict(_constant_factory(1.0/163497))\n",
    "    mapper = ibmModel1Train(english, dutch, mapper, iterations)\n",
    "    print(\"IBMModel 1 training done...\")\n",
    "\n",
    "    final_map, final_align = ibmModel2Train(english, dutch, mapper, iterations)\n",
    "    print(\"IBMModel2 training done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIBM1_2Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTranslations():\n",
    "    \"\"\"\n",
    "    Get the translations from trained model, get the first translation with highest probability\n",
    "    Args: None\n",
    "    Returns Translations from english to dutch\n",
    "    \"\"\"\n",
    "    x = dill.load(open(\"dumps/new_map2_300000_25.pickle\", \"rb\"))\n",
    "    my_dict_prob = {}\n",
    "    my_dict = {}\n",
    "    for key, val in x.items():\n",
    "        if key[0] in my_dict_prob:\n",
    "            if my_dict_prob[key[0]] < val:\n",
    "                my_dict_prob[key[0]] = val\n",
    "                my_dict[key[0]] = key[1]\n",
    "        else:\n",
    "            my_dict_prob[key[0]] = val\n",
    "            my_dict[key[0]] = key[1]\n",
    "    return my_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
