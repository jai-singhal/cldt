{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the essential libraries/\n",
    "import collections\n",
    "from decimal import Decimal\n",
    "import dill as pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************************************\n",
    "*****************\n",
    "\n",
    "# Training ----------------------------------------------------------------\n",
    "\n",
    "***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBAL_VARIABLES\n",
    "engFileName='final_eng.txt'\n",
    "dutchFileName='final_dutch.txt'\n",
    "inputEnglishDataset = \"./English_Updated.txt\" #src\n",
    "inputdutchDataset = \"./Dutch_Updated.txt\"\n",
    "size=100000\n",
    "iterations = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Tokenizes the sentence into list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Converts sentence to list of words:\n",
    "    Args: sentence: str\n",
    "    Returns list of words\n",
    "    \"\"\"\n",
    "    consonants = \"bcdfghjklmnpqrstvwxyz\"\n",
    "    # remove the punctuaations, just take the [a-zA-Z0-9]+ type of regex\n",
    "    words=re.split(r'[` \\t\\-=~!@#$%^&*()_+\\[\\]{};\\\\\\:\"|<,./<>?,\\n\\']', sentence)\n",
    "    output = list()\n",
    "    for w in words:\n",
    "        # remove the consonetnts if any\n",
    "        if len(w) == 1 and w.lower() in list(consonants):\n",
    "            continue\n",
    "        if w in [''] and w.isdigit():\n",
    "            continue\n",
    "        w = w.strip()\n",
    "        output.append(w.lower())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the input files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please mention the file correctly!! Current filename: ./English_Updated.txt\n",
      "Something went wrong\n"
     ]
    }
   ],
   "source": [
    "def cleaningInputFiles():\n",
    "    \"\"\"\"\n",
    "    Takes only [a-zA-Z]+ regex words from the sentences\n",
    "    Opens the file and creates new file as \n",
    "    \"\"\"\n",
    "    def readFile(filename):\n",
    "        sentences = list()\n",
    "        try:\n",
    "            with open(filename, \"r\") as fo:\n",
    "                sentences = fo.readlines()\n",
    "            return sentences\n",
    "        except FileNotFoundError:\n",
    "            print(\"Please mention the file correctly!! Current filename: {}\".format(filename))\n",
    "            raise FileNotFoundError\n",
    "        \n",
    "    def clean(outfileName, sen_list):\n",
    "        with open(outfileName, \"w\") as outf:\n",
    "            for line in sen_list:\n",
    "                line = re.sub(r'[^a-zA-Z]', \" \", line)\n",
    "                words = [w for w in tokenize(line) if len(w) > 0]\n",
    "                sentence= \" \".join(words)\n",
    "                outf.write(sentence + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        english = readFile(inputEnglishDataset)\n",
    "        dutch = readFile(inputdutchDataset)\n",
    "    except:\n",
    "        print(\"Something went wrong\")\n",
    "        return None\n",
    "        \n",
    "    clean(engFileName, english)\n",
    "    clean(dutchFileName, dutch)\n",
    "\n",
    "# run the cleaning first\n",
    "cleaningInputFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifiying the collections.defaultdict\n",
    "class customDict(collections.defaultdict):\n",
    "    def __missing__(self, key):\n",
    "        if self.default_factory is None:\n",
    "            raise KeyError(key)\n",
    "        else:\n",
    "            ret = self[key] = self.default_factory(key)\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux(key):\n",
    "    eng_ptr, du_ptr, eng_len, du_len = key\n",
    "    return (1.0/du_len)\n",
    "\n",
    "def _constant_factory(value):\n",
    "    return lambda: value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IBM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibmModel1Train(english, dutch, transition_prob, iterations):\n",
    "    print(\"In Model1....\")\n",
    "    for i in range(iterations):\n",
    "        print(\"Running Iteration {}.......: \".format(i+1))\n",
    "        count=collections.defaultdict(float)\n",
    "        total=collections.defaultdict(float)\n",
    "        count=collections.defaultdict(float)\n",
    "\n",
    "        sum_total={}\n",
    "\n",
    "        for(en,du) in zip(english, dutch):\n",
    "            eng = tokenize(en)\n",
    "            dut = tokenize(du)\n",
    "            for e in english:\n",
    "                sum_total[e]=0.0\n",
    "                for f in dutch:\n",
    "                    sum_total[e]+=transition_prob[(e,f)]\n",
    "\n",
    "            for e in eng:\n",
    "                for f in dut:\n",
    "                    count[(e, f)] += transition_prob[(e, f)] / sum_total[e]\n",
    "                    total[f]+=transition_prob[(e,f)]/sum_total[e]\n",
    "\n",
    "        for (e,f) in count.keys():\n",
    "            transition_prob[(e,f)]=count[(e,f)]/total[f]\n",
    "        \n",
    "        outfile = open('output/new_map1_{}_{}.pickle'.format(size, i+1), 'wb')\n",
    "        pickle.dump(transition_prob, outfile)\n",
    "\n",
    "    return transition_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IBM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibmModel2Train(english, dutch, mapper, iters):\n",
    "    align = customDict(aux)\n",
    "    print(\"In Model1....\")\n",
    "    for x in range(iters):\n",
    "        print(\"Running Iteration: {}.....\".format(x+1))\n",
    "        count_map = collections.defaultdict(float)\n",
    "        count_align = collections.defaultdict(float)\n",
    "        total_map = collections.defaultdict(float)\n",
    "        total_align = collections.defaultdict(float)\n",
    "        total_map_s = collections.defaultdict(float)\n",
    "\n",
    "        for (eng, du) in zip(english, dutch):\n",
    "            eng = tokenize(eng)\n",
    "            du = tokenize(du)\n",
    "            eng_len = len(english)\n",
    "            du_len = len(dutch)\n",
    "            for eng_ptr, eng_word in enumerate(eng, 1):\n",
    "                total_map_s[eng_word] = 0\n",
    "                for ptr, word in enumerate(du, 1):\n",
    "                    total_map_s[eng_word] += mapper[(eng_word, word)] * align[(eng_ptr, ptr, eng_len, du_len)]\n",
    "\n",
    "            for eng_ptr, eng_word in enumerate(eng, 1):\n",
    "                for ptr, word in enumerate(du, 1):\n",
    "                    temp = mapper[(eng_word, word)] * align[(eng_ptr, ptr, eng_len, du_len)] / total_map_s[eng_word]\n",
    "                    count_map[(eng_word, word)] += temp\n",
    "                    total_map[word] += temp\n",
    "                    count_align[(eng_ptr, ptr, eng_len, du_len)] += temp\n",
    "                    total_align[(eng_ptr, eng_len, du_len)] += temp\n",
    "\n",
    "        # update mapper\n",
    "        for key in count_map.keys():\n",
    "            try:\n",
    "                mapper[key] = count_map[key] / total_map[key[1]]\n",
    "            except decimal.DivisionByZero:\n",
    "                print('Error at', key)\n",
    "                continue\n",
    "\n",
    "        #update aligment\n",
    "        for key in count_align.keys():\n",
    "            align[key] = count_align[key] / total_align[(key[0], key[2], key[3])]\n",
    "\n",
    "        pickle.dump(mapper, open('output/new_map2_{}_{}.pickle'.format(size, x+1),'wb'))\n",
    "        pickle.dump(align, open('output/new_align_{}_{}.pickle'.format(size, x+1),'wb'))\n",
    "\n",
    "    return (mapper, align)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIBM1_2Model():\n",
    "    english = list()\n",
    "    dutch = list()\n",
    "    with open(engFileName,'r') as inp:\n",
    "        english=inp.readlines()[:size]\n",
    "\n",
    "    with open(dutchFileName,'r') as inp:\n",
    "        dutch=inp.readlines()[:size]\n",
    "\n",
    "    print(\"Training starts for total {} sentences\".format(len(english)))\n",
    "    print(\"Pre-trained IBMModel1 not found..start training\")\n",
    "    mapper = collections.defaultdict(_constant_factory(1.0/163497))\n",
    "    mapper = ibmModel1Train(english, dutch, mapper, iterations)\n",
    "    print(\"IBMModel 1 training done...\")\n",
    "\n",
    "    final_map, final_align = ibmModel2Train(english, dutch, mapper, iterations)\n",
    "    print(\"IBMModel2 training done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go and train the model\n",
    "trainIBM1_2Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************************************\n",
    "*****************\n",
    "\n",
    "# Testing ----------------------------------------------------------------\n",
    "\n",
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Translations\n",
    "\n",
    "Get the translations from trained model, get the first translation with highest probability\n",
    "\n",
    "Returns Translations from english to dutch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTranslations():\n",
    "    \"\"\"\n",
    "    Get the translations from trained model, get the first translation with highest probability\n",
    "    Args: None\n",
    "    Returns Translations from english to dutch\n",
    "    \"\"\"\n",
    "    print(\"Reading Translations.....\")\n",
    "    print(\"Reading the pickle time may takes time....\")\n",
    "\n",
    "    finalTranlationsOut = \"translations.pickle\"\n",
    "    try:\n",
    "        x = pickle.load(open(finalTranlationsOut, \"rb\"))\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found on appropriate location\")\n",
    "        filename = input(\"Input the finalTrained pickle file location: \")\n",
    "        x = pickle.load(open(filename, \"rb\"))\n",
    "    translations_prob = {}\n",
    "    translations = {}\n",
    "    for key, val in x.items():\n",
    "        if key[0] in translations_prob:\n",
    "            if translations_prob[key[0]] < val:\n",
    "                translations_prob[key[0]] = val\n",
    "                translations[key[0]] = key[1]\n",
    "        else:\n",
    "            translations_prob[key[0]] = val\n",
    "            translations[key[0]] = key[1]\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get  cosine similarity and  Jacard coef\n",
    "\n",
    "Get the cosine similarity between 2 documents\n",
    "    Cosine similarity from 0 to 1\n",
    "    \n",
    "Get the Jacard coef between 2 documents\n",
    "    Jacard coef ranges from 0 to 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCosineSimilarity(src, target):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the cosine similarity between 2 documents\n",
    "    Cosine similarity from 0 to 1\n",
    "\n",
    "    Args: Two strings of docs\n",
    "    Returns cosine similarity : float(0, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    l1 =[];l2 =[] \n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in src}  \n",
    "    Y_set = {w for w in target} \n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)\n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i]\n",
    "    try:\n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "        return cosine \n",
    "    except decimal.DivisionByZero:\n",
    "        print(\"Zero error\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "def getJacardCoeficient(src, target):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the Jacard coef between 2 documents\n",
    "    Jacard coef ranges from 0 to 1\n",
    "    Args: Two strings of docs\n",
    "    Returns Jac Coef: float(0, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    d1 = set(src)\n",
    "    d2 = set(target)\n",
    "    d1ud2 = d1.union(d2)\n",
    "    d1id2 = d1.intersection(d2)\n",
    "    return float(len(d1id2))/len(d1ud2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Driver Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    print(\"Welcome to Cross Language Translations(English<->Dutch)\")\n",
    "\n",
    "    averageJC_score = 0\n",
    "    averageCS_score = 0\n",
    "    total_tests = 0\n",
    "    translations_e_d = getTranslations()\n",
    "    translations_d_e = dict([(value, key) for key, value in translations_e_d.items()]) \n",
    "\n",
    "    while True:\n",
    "        print(\"Which Translation you want?\")\n",
    "        print(\"1. ENGLISH TO DUTCH\")\n",
    "        print(\"2. DUTCH TO ENGLISH\")\n",
    "        ch = int(input(\"Please select the option: \"))\n",
    "        \n",
    "        if ch == 1:\n",
    "            translations = translations_e_d\n",
    "        elif ch == 2:\n",
    "            translations = translations_d_e\n",
    "        else:\n",
    "            print(\"You entered wrong choice\")\n",
    "            sys.exit()\n",
    "\n",
    "        src_path = input(\"Enter source document path: \")\n",
    "        trg_path = input(\"Enter target document path: \")\n",
    "        try:\n",
    "            src_file = open(src_path, 'r')\n",
    "            src = src_file.read()\n",
    "        except FileNotFoundError:\n",
    "            print(\"Src file does not exists\")\n",
    "            raise FileNotFoundError\n",
    "            sys.exit()\n",
    "        try:\n",
    "            targ_file = open(trg_path, 'r')\n",
    "            trg = targ_file.read()\n",
    "        except:\n",
    "            print(\"Destination file does not exists\")\n",
    "            raise FileNotFoundError\n",
    "            sys.exit()\n",
    "\n",
    "        src_words = tokenize(src)\n",
    "        trg_words = tokenize(trg)\n",
    "\n",
    "        translated_list = list()\n",
    "        for w in src_words:\n",
    "            if len(w) > 0 and w in translations.keys():\n",
    "                translated_list.append(translations[w])\n",
    "\n",
    "        \n",
    "        cs = getCosineSimilarity(trg_words, translated_list)\n",
    "        jc = getJacardCoeficient(trg_words, translated_list)\n",
    "\n",
    "        print(\"This document has cosine similarity: {}\".format(cs))\n",
    "        print(\"This document has Jacard similarity: {}\".format(jc))\n",
    "\n",
    "        \n",
    "        translated_doc = \" \".join(translated_list)\n",
    "        print(\"Ouputing your result into filename: translated_{}.txt\".format(total_tests+1))\n",
    "        with open(\"translated_{}.txt\".format(total_tests+1), \"w\") as fout:\n",
    "            fout.write(translated_doc)\n",
    "\n",
    "        print(\"Do you want to continue? \")\n",
    "        ch = input(\"Please select the option(Y/N)?: \")\n",
    "        total_tests += 1\n",
    "        averageCS_score += cs\n",
    "        averageJC_score += jc\n",
    "\n",
    "        if ch == 'Y' or ch == 'y':\n",
    "            #normal exn\n",
    "            print(\"Current Average cosine similarity: {}\".format(cs/total_tests))\n",
    "            print(\"Current Average Jacard similarity: {}\".format(jc/total_tests))\n",
    "        elif ch == 'N' or ch == 'n':\n",
    "            # print the average coefficent score\n",
    "            print(\"Final Average cosine similarity: {}\".format(cs/total_tests))\n",
    "            print(\"Final Average Jacard similarity: {}\".format(jc/total_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Cross Language Translations(English<->Dutch)\n",
      "Reading Translations.....\n",
      "Reading the pickle time may takes time....\n",
      "Which Translation you want?\n",
      "1. ENGLISH TO DUTCH\n",
      "2. DUTCH TO ENGLISH\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
